{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fcf496",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec6ad1a3b11de3d9eab49b5f5eaacbac",
     "grade": false,
     "grade_id": "cell-8a8cd8b757aa43d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Algorithms of the Mind\n",
    "\n",
    "**Instructions:** Answer all questions below. Be sure to show all intermediate steps and equations that you used to arrive at each answer. Please type your answers (including your equations). For coding questions, your code and its execution will do.\n",
    "\n",
    "**How to submit?:** Execute all blocks of your Jupyter notebook, save it, and submit your assignment using Canvas.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <strong>Note</strong>\n",
    "\n",
    "Your answers in each question can be a combination of markdown and Julia code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b362fa3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b32401381437e041d3866a9c8f7517bb",
     "grade": false,
     "grade_id": "cell-0cdff38fa9dd89b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a35b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "NETID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16c1d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "862c67f6e7489bc2c56f69c85f568715",
     "grade": false,
     "grade_id": "cell-24c93db6109b5e4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Next, please take the honor pledge by reordering the following phrases so that it makes sense to you, and then typing the resulting full sentence.\n",
    "\n",
    "- and that this work is my own.\n",
    "- or received\n",
    "- I have not given\n",
    "- I affirm that\n",
    "- on this assignment,\n",
    "- any unauthorized help "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da0ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HONOR_PLEDGE = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a6686",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8112be3d89ed34ec1d73eed2ccd2001",
     "grade": true,
     "grade_id": "cell-f58af99f51780260",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df01038",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3a0bd-d83e-412d-933b-52930e8b21c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0285a69e60e93e04fb5372fdadee90fe",
     "grade": false,
     "grade_id": "cell-4144138019c9cd3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem Set 4\n",
    "\n",
    "This problem set will be unusual in that it'll be half a lab section and half a problem set.\n",
    "\n",
    "The first half (or so) of the problem set will feel lab-like because it introduces creating and solving MDPs using the `POMDPs.jl` package (an incredibly done package!).\n",
    "Critically, this lab-like structure will be necessary for the remainder of the problem set, and you will receive many of your points merely for going through the material.\n",
    "\n",
    "<!-- It'll feel like a lab section in that there is a good portion of the problem set that introduces how you can create and solve a markov decision problem using the POMDPs.jl package in Julia (a very well done package!). Critically, this will be worth the effort: Rest of the problem set depends on it, and you will get a good chunk of your points for merely going through that material. -->\n",
    "\n",
    "The second half (or so) of the problem set will feel more pset-like as you will be tasked with creating generative models and making inference using it.\n",
    "Like previous problem sets, you will be graded on these models and your inferences as well.\n",
    "\n",
    "Enjoy the problem set &ndash; we think it'll be fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c8ad7-b2d7-4c08-bb8b-c4b4a2e8cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"psyc261\")\n",
    "Pkg.add([\n",
    "    \"Distributions\",\n",
    "    \"POMDPs\",\n",
    "    \"POMDPTools\",\n",
    "    \"POMDPPolicies\",\n",
    "    \"POMDPSimulators\",\n",
    "    \"DiscreteValueIteration\",\n",
    "    \"Compose\",\n",
    "    \"StaticArrays\",\n",
    "    \"ColorSchemes\",\n",
    "])\n",
    "# load necessary packages for this problem set\n",
    "# Note that running this for the first time might take a good 15 mins; plan ahead\n",
    "using Random\n",
    "using Gen\n",
    "using Plots\n",
    "using DelimitedFiles\n",
    "using POMDPs, POMDPTools, POMDPPolicies, POMDPSimulators\n",
    "using Compose\n",
    "using StaticArrays\n",
    "using ColorSchemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ca847-d2f3-4128-b5f9-f195576c1920",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdb3b2f1c395c972d8e83a1ce53f7cc0",
     "grade": false,
     "grade_id": "cell-c69357dee44667b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1: Inferring rewards in an MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545f401-6054-4603-9e54-dc57fe76ae76",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e2ce9ecd54e61f0c2eb26d25120b9b7",
     "grade": false,
     "grade_id": "cell-717b47ed22dcbaa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will work on a maze-navigation problem in a grid-world domain.\n",
    "As this is a grid-world MDP, the agent **fully observes** its location (a coordinate-pair).\n",
    "The agent may stochastically move in one of the cardinal directions.\n",
    "Lastly, the agent may receive a positive or negative reward for entering particular states.\n",
    "\n",
    "In more technical terms, the MDP's $\\langle \\mathcal{S, A, T, R} \\rangle$ tuple is:\n",
    "- $\\mathcal{S}$: each grid-world cell,\n",
    "- $\\mathcal{A}$: are cardinal directions (`N`, `E`, `S`, `W`).\n",
    "- $\\mathcal{T}$: transitions are stochastic ($0.9$ probability of moving as intended, $0.1$ probability of moving randomly),\n",
    "- $\\mathcal{R}$: cells may either contain $0$ reward, a positive reward, or a negative reward.\n",
    "\n",
    "As an observer, we watch the agent's actions, we know the locations of the rewards, and know the values of the negative rewards; **however**, we _do not know the values of the **positive rewards**_.\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\">\n",
    "\n",
    "Your goal is to infer the positive reward values from the agent's actions.\n",
    "</div>\n",
    "\n",
    "We can formalize this maze-navigation problem using the MDP framework to setup a generative model of agents, where our latent random variables will be the positive reward values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a61479-4ff7-4deb-a341-1593a569fad8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80dcea31642ea1256d81a59a6e0c2f07",
     "grade": false,
     "grade_id": "cell-8bac9a7c1c179002",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1A [5 pts] Defining the MDP for our grid world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bafb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ea3cd88120d19c9af3bb1e1d628eafe",
     "grade": false,
     "grade_id": "cell-fc1df231b650e60a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <strong>Note:</strong>\n",
    "\n",
    "You will not be asked to write code for this question. Your job is to process the following material (in conjuction with the tutorial linked below, as needed) and respond to a brief comprehension question at the end.\n",
    "</div>\n",
    "\n",
    "We will use [`POMDPs.jl`](https://github.com/JuliaPOMDP/POMDPs.jl) to setup our MDP and solve it (using the Value Iteration algorithm). You can find documentation for `POMDPs.jl` [here](https://juliapomdp.github.io/POMDPs.jl/stable/).\n",
    "\n",
    "We will closely follow [one of the tutorials provided by `POMDPs.jl`][gw-tutorial] (which you are free to consult as you like).\n",
    "\n",
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <strong>Note:</strong>\n",
    "\n",
    "The tutorial above references some of `POMDPs.jl`'s older packages/APIs, we have taken steps to ensure that our notebook uses the correct, and latest, APIs/packages &ndash; so the tutorial should be referred to primarily for understanding, rather than checking your code against.\n",
    "</div>\n",
    "\n",
    "[gw-tutorial]: https://github.com/JuliaPOMDP/POMDPExamples.jl/blob/master/notebooks/GridWorld.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66484809",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60c6a374fbdb8d48e469fbff1427dca0",
     "grade": false,
     "grade_id": "cell-8a0cb3a672b57b85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, we will create a `struct` to represent the world state. Notice that this struct contains information only about the aspects of the grid world that changes at each time step: the location of the agent and whether it is at a terminal state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce30036-1b16-4e9e-a226-2de07174a23b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f218d725ecdd09923119fd7e234bcd4",
     "grade": false,
     "grade_id": "cell-1e8831c7d3b5b3f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "const Point = SVector{2,Int}\n",
    "\n",
    "struct GridWorldState\n",
    "    pos::Point  # an (x, y) coordinate pair\n",
    "    done::Bool  # are we in a terminal state?\n",
    "end\n",
    "\n",
    "# a helper function as a state constructor\n",
    "GridWorldState(x::Int, y::Int) = GridWorldState(Point(x, y), false)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572c63e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3b82c475be162b6a2576db8940c4cfe",
     "grade": false,
     "grade_id": "cell-b6966dd548d3d9d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we will create a `struct` to represent our actions.\n",
    "This is primarily for syntactic sugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040eb57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7a2c7bb9e0a37a131488b27e9a31e6a",
     "grade": false,
     "grade_id": "cell-b216972b8dc72de1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "struct GridWorldAction\n",
    "    pos::Point  # an (x, y) coordinate pair, exactly like in `GridWorldState`\n",
    "end\n",
    "\n",
    "# a helper function as an action constructor\n",
    "GridWorldAction(x::Int, y::Int) = GridWorldAction(Point(x, y))\n",
    "\n",
    "const N = GridWorldAction(0, 1)\n",
    "const E = GridWorldAction(1, 0)\n",
    "const S = GridWorldAction(0, -1)\n",
    "const W = GridWorldAction(-1, 0)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27004107-5ea6-4db7-aff4-612909566cb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aa1b8b2a5c52eddd90199ffdd673410",
     "grade": false,
     "grade_id": "cell-540e7ce62ed1c9e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we define our MDP problem using a `mutable struct`.\n",
    "To do this, our `mutable struct` must inherit from the `MDP` type and we need to ensure that the \"state\" and \"action\" types are present as well.\n",
    "\n",
    "Our `GridWorld` contains the dimensions of the grid-world (`size_x`, `size_y`) (which determines the size of the state space), a list of states with some reward (`reward_states`), and the reward values for each of those states (`reward_values`).\n",
    "\n",
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <strong>Note</strong>\n",
    "\n",
    "States with a positive reward are designed to be terminal nodes. After visiting one of these states, the agent cannot further accumulate reward.\n",
    "</div>\n",
    "\n",
    "Additionally, the `tprob` parameter controls the stochasticity of the environment: specifically, `tprob` is the probability the agent's intended action succeeds (e.g., a move `N` indeed moves `:N`), while `1 - tprob` is the probability the agent moves randomly.\n",
    "In this problem set, we set `tprob = 0.9`.\n",
    "This stochasticity will be an important variable to account for when writing our likelihood function in [Q1B](#q1b).\n",
    "\n",
    "Lastly, our MDP (as with all MDPs) must include a `discount_factor` ($\\mathcal{\\gamma}$).\n",
    "Discount factors (or rates) can be used to incentivize agents to maximize reward over shorter paths.\n",
    "For our purposes, we will use `discount_factor = 0.9`, this means the future reward estimates will be diminished by 10% for each step into the future they are. (This is much like placing a negative reward on each cell for simply taking a step, as discussed in the lecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947a42e-3989-4437-82db-e2f159ff69e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aabcadf8801911a32d8586cc20f912cb",
     "grade": false,
     "grade_id": "cell-e20135db3a2bde99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the Grid World MDP\n",
    "# Note that our MDP is parameterized by the state and action\n",
    "mutable struct GridWorld <: MDP{GridWorldState,GridWorldAction}\n",
    "    size_x::Int # x size of the grid\n",
    "    size_y::Int # y size of the grid\n",
    "    reward_states::Vector{GridWorldState} # the states in which agent recieves reward\n",
    "    reward_values::Vector{Real} # reward values for those states\n",
    "    tprob::Real # probability of transitioning to the desired state\n",
    "    discount_factor::Real # disocunt factor\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d63d6-55c7-4403-9d0a-5a4a8881386c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c09508f0a120816c1e535e5d377472b7",
     "grade": false,
     "grade_id": "cell-1290bf56c836a4d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we will define a helper function to construct a GridWorld MDP with some default values, and then call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112b424-97c7-448f-b1e0-71a505846b54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bcf9b54850abc9ad4e5f2df9863b8f1",
     "grade": false,
     "grade_id": "cell-cbbe94757bcceaf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#we use key worded arguments so we can change any of the values we pass in \n",
    "function GridWorld(;\n",
    "    sx::Int=10, # size_x\n",
    "    sy::Int=10, # size_y\n",
    "    rs::Vector{GridWorldState}=[  # reward states\n",
    "        GridWorldState(Point(4, 3), true),\n",
    "        GridWorldState(Point(4, 6), true),\n",
    "        GridWorldState(Point(8, 2), true),\n",
    "        GridWorldState(Point(4, 9), true),\n",
    "    ],\n",
    "    rv::Vector{<:Real}=[-10.0, -5.0, 5.0, 5.0], # reward values\n",
    "    tp::Real=0.9, # tprob\n",
    "    discount_factor::Real=0.9\n",
    ")\n",
    "    return GridWorld(sx, sy, rs, rv, tp, discount_factor)\n",
    "end\n",
    "\n",
    "# we can now instantiate a GridWorld `mdp` like this:\n",
    "mdp = GridWorld(; rv=[-10, -5.0, 2, 10]);\n",
    "# you can view the reward states by uncommenting the following line:\n",
    "#mdp.reward_states # mdp contains all the defualt values from the constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8aea6-c52e-40b4-844e-6567e13fb24e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5ebcecd3cb0b2c2f3f6cd6a2cabe233",
     "grade": false,
     "grade_id": "cell-429ae7f82d5d2d8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now with the `mdp::GridWorld` at hand, we can fully specify the tuple $\\langle \\mathcal{S, A, T, R} \\rangle$: state space, actions, transition function, and rewards.\n",
    "The file `mdp.jl` defines each of these using the `POMDPs.jl` \"object-oriented\" interface (this is typically how MDPs are defined using `POMDPs.jl`).\n",
    "Please review this file.\n",
    "We also include the file `viz.jl` for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd3342-05c8-460c-bbff-d5f02895df32",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acc859c4b44bd592fcdb60d30dfc0825",
     "grade": false,
     "grade_id": "cell-7675cbbe5139b0cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#incude mdp.jl to define states, actions, transitions, and rewards. \n",
    "include(\"utils/mdp.jl\");\n",
    "include(\"utils/viz.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256dd11-7f69-4e14-a552-6ccdbbd7d738",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab247ebcd14075e53e5ad856baccb86e",
     "grade": false,
     "grade_id": "cell-aee5e2573a8b2dda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's take a few deterministic actions and visualize the grid world and the actions of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b020cab-84b6-4271-bf54-3e2f8a89a443",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9ea3fa5b5de7427957e30f91204e1e2",
     "grade": false,
     "grade_id": "cell-f5514239d6cd34db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "step = Dict()\n",
    "step[:start] = Point(1, 1)\n",
    "step[:actions] = [N, E, E]\n",
    "render(mdp, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6990ba1b-0598-4e5d-ad5e-164f27d42393",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "939153a6f82de8bed20d5299966feb80",
     "grade": false,
     "grade_id": "cell-9d6b424fd9da0253",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we will initialize our agent in the MDP, placing it at coordinates [1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0847a1-cbb6-485c-bec6-50a8f8fa5e37",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69bc50974b5b428dc0f00448efdc86a9",
     "grade": false,
     "grade_id": "cell-261c47afe5d462a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "POMDPs.initialstate(mdp::GridWorld) = Deterministic(GridWorldState(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b50066-e219-4e3f-9cc4-5fc5df0efc8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e07fed739042beef5b13409152bf8cc8",
     "grade": false,
     "grade_id": "cell-5e7dddbc77f4bbc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we will simulate the behavior of the agent with random policy: take random actions at each time step. You can see the agent meandering, including hitting against the boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c189037-ebd6-427b-8acf-ad04fd27deb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24b85b75d2273a001f94d9607e52f189",
     "grade": false,
     "grade_id": "cell-bdeab2fe687a7eb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mdp = GridWorld()\n",
    "\n",
    "random_policy = RandomPolicy(mdp)\n",
    "\n",
    "for (s, a, r) in stepthrough(mdp, random_policy, \"s,a,r\", max_steps=10)\n",
    "    @show s\n",
    "    @show a\n",
    "    @show r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf3518-e2df-4580-af96-5c8764b7edb2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd9170155c72876c08cc5934c5808e40",
     "grade": false,
     "grade_id": "cell-80a0524a30442add",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we are ready to solve our MDP, for which we will use the `ValueIterationSolver` from the [DiscreteValueIteration](https://github.com/JuliaPOMDP/DiscreteValueIteration.jl) package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04672a9-30be-49cc-84ff-e2ff92272230",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "835c46e308c8b51c863d73f4a3a44008",
     "grade": false,
     "grade_id": "cell-94c2d99d129f6752",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# first let's load DiscreteValueIteration\n",
    "using DiscreteValueIteration\n",
    "\n",
    "# initialize the problem \n",
    "mdp = GridWorld()\n",
    "\n",
    "# Try a slightly different initialization of the positive rewards and see how it impacts the behavior of the agent \n",
    "# mdp = GridWorld(rv=[-10.0, -5.0, 2, 10.0])\n",
    "\n",
    "# initialize the solver\n",
    "# max_iterations: maximum number of iterations value iteration runs for (default is 100)\n",
    "# belres: the value of Bellman residual used in the solver (defualt is 1e-3)\n",
    "solver = ValueIterationSolver(max_iterations=100, belres=1e-3; verbose=true)\n",
    "\n",
    "# solve for an optimal policy\n",
    "policy = solve(solver, mdp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349919a5-f1e8-4a4d-81f4-b42b58d37593",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4eee2bb6ebc930787b7900c4f2c0e93b",
     "grade": false,
     "grade_id": "cell-8c8df593cd8efb9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now with our `policy` at hand, we can take actions using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8183c93-6c9f-44b9-91fb-cc66bdbb56b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3872d5bde519716c7a4d805db2fb301",
     "grade": false,
     "grade_id": "cell-f87b5dba2f6caf06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "POMDPs.initialstate(mdp::GridWorld) = Deterministic(GridWorldState(1, 1))\n",
    "# record steps for visualization\n",
    "step = Dict()\n",
    "step[:start] = (1, 1)\n",
    "step[:actions] = []\n",
    "step[:visitedstates] = []\n",
    "push!(step[:visitedstates], (1, 1))\n",
    "for (s, a, r) in stepthrough(mdp, policy, \"s,a,r\", max_steps=10)\n",
    "    push!(step[:actions], a)\n",
    "    push!(step[:visitedstates], (s.pos.x, s.pos.y))\n",
    "end\n",
    "# visualize the world and the actions of the agent\n",
    "render(mdp, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac809d4-4356-439f-b1f7-eed36fc81d54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05e8cbc13085ce2c1e94c183b44bde62",
     "grade": false,
     "grade_id": "cell-77aba44f8b4df660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can appreciate a much more goal-oriented behavior with the optimal policy.\n",
    "\n",
    "Before we turn to the questions, there are two more critical functions we need to learn about in the `POMDPs.jl` package.\n",
    "\n",
    "The function `action` consumes a `policy::Policy` and a current state (`state::GridWorldState` in our case). It returns an action (`action::GridWorldAction` in our case) under the given `policy`. (`action` is called, internally, by `stepthrough`.)\n",
    "\n",
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <strong> Note </strong>\n",
    "\n",
    "The `action` function will only return the reward-maximizing `action` to take. \n",
    "\n",
    "</div>\n",
    "\n",
    "Lastly, the function `value` consumes a `policy::Policy`, `state`, and `action` to compute the expected reward by taking the `action` in the given `state`.\n",
    "_We will use `value` to create our Boltzman (or softmax) policy in our generative model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8a6f3-47ce-4cf0-81ae-abadcec4e168",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a008ca74d4ed6d8af8abe716bb33e8ca",
     "grade": false,
     "grade_id": "cell-b5279f4ec2e46a67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# say we are in state (9,2)\n",
    "s = GridWorldState(9, 2)\n",
    "a = action(policy, s)\n",
    "println(a)\n",
    "value(policy, s, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5d34d-a6bb-4b61-8dcc-ecbfd43b50c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "005642251bf3083f147b63cb2099585d",
     "grade": false,
     "grade_id": "cell-ffc5630980d7db95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write down the probability distribution that the transition function specifies in an MDP and describe in a sentence or two how `tprob` relates to the MDP's transition function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a35096-691e-40ff-871e-7ae6be469ebe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f904a8db89bc1a366c25731b9cc530d",
     "grade": true,
     "grade_id": "cell-78079e903e590fe8",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab4c83-3306-495f-98c1-a063c66dfb56",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f74ba856504d541f23b2dba5c7a4a98",
     "grade": false,
     "grade_id": "cell-c6b7895adab053c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1B: [10 pts] Generative agent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a780bb1-91cf-4f46-8108-bfb56c4d69c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b0c39f7f84d9410f21cab5c46f1b342",
     "grade": false,
     "grade_id": "cell-e1e94dbfd8591446",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a generative model, called `agent_model`, that assumes the knowledge of the grid world defined above, except for the values of the positive rewards.\n",
    "\n",
    "Place uniform (discrete) priors over these rewards, between 1 and 10.\n",
    "\n",
    "Use the variable name `north_reward` and `south_reward` to refer to the two rewards with positive values on our grid world.\n",
    "\n",
    "You must keep the values and locations of the negative rewards same as defined above. You must also keep the locations of the positive rewards same as above. \n",
    "\n",
    "Use the `ValueIterationSolver` to obtain a policy, and define your likelihood function by drawing actions following a Boltzman policy. Use a temperature of $\\beta=1.5$. You can use the following function to turn the value vector into a softmax probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d58f5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fca3f908833c9b31aa2ad1303f9d321a",
     "grade": false,
     "grade_id": "cell-81548979a559141e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "include(\"utils/draw.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611141da-4985-4f2e-bdbd-13174f766ad2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e1be5ba7c9250cecbece4f74dea9e16",
     "grade": false,
     "grade_id": "cell-c6f979dab58cedc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "function softmax(score; beta=1.5)\n",
    "    score = score .- maximum(score)\n",
    "    score = score ./ beta # temperature\n",
    "    exp_score = exp.(score)\n",
    "    return exp_score ./ sum(exp_score)\n",
    "end\n",
    "\n",
    "# we need to initialize the MDP in global context.\n",
    "POMDPs.initialstate(mdp::GridWorld) = Deterministic(GridWorldState(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cb82d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b27c3401ccc910f2faf95c48cc3f0f97",
     "grade": false,
     "grade_id": "cell-76f9a59556548a56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <strong> Note </strong>\n",
    "\n",
    "\n",
    "We use `Gen.@gen` (rather than `@gen`) because `POMDPs.jl` also exports an `@gen` macro.\n",
    "Since we imported `Gen` via `using Gen`, we can prefix `Gen` to `@gen`, thus ensuring that our `agent_model` is parsed by `Gen.jl`, rather than `POMDPs.jl`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066f2a5-647c-4557-917f-38db7c91e869",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22259badba9972534a13404369cba174",
     "grade": true,
     "grade_id": "cell-c95632cfcf0fba07",
     "locked": false,
     "points": 9,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Gen.@gen function agent_model(maxsteps=10, beta=1.5)\n",
    "    # draw random variables\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "    # create the mdp with these random decisions\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "    # use Value Iteration as your solver, and solve the MDP to get your policy\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "    # We will use the dictionary step to keep track of the states and actions for visualization purposes\n",
    "    step = Dict()\n",
    "    step[:start] = (1, 1)\n",
    "    step[:actions] = []\n",
    "    step[:visitedstates] = []\n",
    "    push!(step[:visitedstates], (1, 1))\n",
    "\n",
    "    for (k, (s, a, r)) in enumerate(stepthrough(mdp, policy, \"s,a,r\", max_steps=maxsteps))\n",
    "        # your code goes here, defining a Boltzman policy (keep beta=1.5). \n",
    "        # Your code should yield a probability vector over actions, called \"prob_vector\"\n",
    "        # your code here\n",
    "        throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "        # Draw an action from the policy\n",
    "        # your code here\n",
    "        throw(Exception(\"Not Implemented.\"))\n",
    "\n",
    "        # save the action and state\n",
    "        # your code here\n",
    "        throw(Exception(\"Not Implemented.\"))\n",
    "    end\n",
    "\n",
    "    return step, mdp\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4217f5b-7c90-4199-9b68-aae0886f3640",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b366557e3e08063a0a353dbb21eb9c6c",
     "grade": false,
     "grade_id": "cell-f3037a2bf134ca94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Draw a sample from your generative model, using the Gen.generate function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3552b-c2fe-4f52-945e-ad9ebb8371c2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec7012eb1d9b795ad9a09bd00b50b5dd",
     "grade": true,
     "grade_id": "cell-acbe14a8ca864193",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))\n",
    "get_choices(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4aef3-3c85-4644-932b-962c8babe780",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fa96eb6d0419e980ba0845c4c704c97",
     "grade": false,
     "grade_id": "cell-a3ba0944cc49e67a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, we provide a visualization code by utilizing the values we recorded and returned in the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12eb91e-14fb-4b5a-b224-0c19feb03eba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0b975cf4d1678e06cd580313e83c8f5",
     "grade": false,
     "grade_id": "cell-5145111d5a53da20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "history_, mdp_ = get_retval(trace)\n",
    "render(mdp_, history_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020da24-fda1-4e2a-a990-af7efeb1dde9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92df8b01ccefc4f52319f2e4c2d433c2",
     "grade": false,
     "grade_id": "cell-ab311932642c5215",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1C: [10pts] Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a826d-7daa-47b7-8cb0-35b7c4e4bebf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4083970d2fffcb77cfccbd8f126b415e",
     "grade": false,
     "grade_id": "cell-27f6ee6d624715fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section, we will pose an inference problem to our generative model; you will write an importance sampler and comment on the posterior it infers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83911971-b15e-41d6-9e4b-c2e7bced1681",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8501f44085b0c8ccb3d70b46744ad6cf",
     "grade": true,
     "grade_id": "cell-5f22ff423c273769",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# a helper function to make an observation choicemap.\n",
    "function get_observations(trace; maxsteps=10)\n",
    "    # Constrain the observed measurements.\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d7ca47-2401-4085-bdb2-2ab3df0d8c66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "758f88c1d471f4be42fc49483e8a09e8",
     "grade": false,
     "grade_id": "cell-e2ed16da5575cf57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You will pose an inference problem by synthesizing an action sequence from our generative model.\n",
    "Sample a trace (`trace`) from your generative model where the `south_reward` is `3` and the `north_reward` is `7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9d7b3-43d8-4d05-bcaf-1a0e57b9bde0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edeb7075baec4d18f32caf0a2ad699e0",
     "grade": true,
     "grade_id": "cell-f872ca2fe071b8cd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ade43-5f7b-44ff-8260-e5758fc97120",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05f65b8083ca8391acd7d8c3af8364cc",
     "grade": false,
     "grade_id": "cell-802f99f347e299e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Call the `get_observations` function to get a choicemap of the actions taken in the stimulated trace, called `observations`.\n",
    "\n",
    "Then, visualize the trace.\n",
    "\n",
    "<details class=\"alert alert-info\" markdown=\"1\">\n",
    "    <summary><strong>Hint</strong></summary>\n",
    "\n",
    "Similar visualizations have been provided in this problem set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f04b6-7486-4f3b-a4b1-d16fe06535a9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b891126081492d8eb7534553e6633bae",
     "grade": true,
     "grade_id": "cell-e77eb0c3f825fa0b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589b593-a681-48c8-a39c-1a22c76b70c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0440868973e38703d849e9c19d4e8a53",
     "grade": false,
     "grade_id": "cell-f9d7a2fd561dc3f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now make inference using importance sampling by filling in the `do_inference` function. Return both the trace and its score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9119eff-2cd8-49ec-a0ee-5d8b0f78905a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f1f32b52ab3777398ef92fb15d4bad1",
     "grade": true,
     "grade_id": "cell-49ef8efd3da8312d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "function do_inference(amount_of_computation=500)\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    return (trace, score)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1995a29-c5f7-40ea-9043-428d3fdfeb82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98c288ac261099fee6b5e9ae3342944f",
     "grade": false,
     "grade_id": "cell-ea0e7e4613af1ffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Fill in the next code block to run this inference procedure for 5 chains, print out the positive reward values for each chain, and visualize the chain with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eddff7-b5ed-452d-85e6-a38142185276",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "670275a7b584e2c201dcd7743bcc3658",
     "grade": true,
     "grade_id": "cell-5410163276f75639",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bestscore = -Inf\n",
    "inferred_rewards = []\n",
    "besttrace = nothing\n",
    "for sample = 1:5\n",
    "    # Call the do_inference functions\n",
    "    # your code here\n",
    "    throw(Exception(\"Not Implemented.\"))\n",
    "    push!(inferred_rewards, (trace[:south_reward], trace[:north_reward]))\n",
    "    if score > bestscore\n",
    "        besttrace = trace\n",
    "        bestscore = score\n",
    "    end\n",
    "end\n",
    "\n",
    "println(inferred_rewards)\n",
    "\n",
    "# visualize\n",
    "# your code here\n",
    "throw(Exception(\"Not Implemented.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc53b72-eb2c-4659-a590-cf0af665721f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6beb54a2b03a413012764b537aa5cbde",
     "grade": false,
     "grade_id": "cell-2b5dae600bd92095",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Comment on your posterior with 2 sentences: in what way does your posterior make sense? Your answer should consider the ill-posed nature of this inference problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fed172-fb9b-4fc1-b760-39d6b1692f5c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d656f72919b114bd5b808e020dcbe793",
     "grade": true,
     "grade_id": "cell-f80f883f97b5f576",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b24be-b8bb-4866-84f9-33ba272d23df",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (AotM) 1.9.2",
   "language": "julia",
   "name": "julia-_aotm_-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
